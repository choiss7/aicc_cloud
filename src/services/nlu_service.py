"""
AWS Connect ì½œì„¼í„°ìš© ìì—°ì–´ ì´í•´(NLU) ì„œë¹„ìŠ¤
"""
import json
import logging
from typing import Dict, List, Optional, Any, Tuple
import boto3
from botocore.exceptions import ClientError
from datetime import datetime
import re

from ..chatbot_nlu import ChatbotNLU, NLUResponse, IntentResult

logger = logging.getLogger(__name__)

class NLUService:
    """ìì—°ì–´ ì´í•´ ì„œë¹„ìŠ¤"""
    
    def __init__(self, lex_bot_name: str, dynamodb_table_name: str = "nlu_analytics"):
        self.nlu_engine = ChatbotNLU(lex_bot_name)
        
        # DynamoDB for analytics and training data
        self.dynamodb = boto3.resource('dynamodb')
        self.analytics_table = self.dynamodb.Table(dynamodb_table_name)
        self.training_table = self.dynamodb.Table(f"{dynamodb_table_name}_training")
        
        # Comprehend for sentiment analysis
        self.comprehend = boto3.client('comprehend')
        
        # CloudWatch for metrics
        self.cloudwatch = boto3.client('cloudwatch')
        
        # ê°ì • ë¶„ì„ ì„¤ì •
        self.sentiment_threshold = {
            'positive': 0.7,
            'neutral': 0.5,
            'negative': 0.3
        }
        
        # ì–¸ì–´ ê°ì§€ ì„¤ì •
        self.supported_languages = ['ko', 'en', 'ja', 'zh']
        self.default_language = 'ko'
    
    def process_user_input(self, user_input: str, session_id: str,
                          session_attributes: Optional[Dict] = None,
                          include_sentiment: bool = True) -> Dict[str, Any]:
        """
        ì‚¬ìš©ì ì…ë ¥ì„ ì¢…í•©ì ìœ¼ë¡œ ì²˜ë¦¬
        
        Args:
            user_input: ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
            session_id: ì„¸ì…˜ ID
            session_attributes: ì„¸ì…˜ ì†ì„±
            include_sentiment: ê°ì • ë¶„ì„ í¬í•¨ ì—¬ë¶€
            
        Returns:
            Dict: ì²˜ë¦¬ ê²°ê³¼
        """
        try:
            # ì…ë ¥ ì „ì²˜ë¦¬
            preprocessed_input = self._preprocess_input(user_input)
            
            # ì–¸ì–´ ê°ì§€
            detected_language = self._detect_language(preprocessed_input)
            
            # ê¸°ë³¸ NLU ì²˜ë¦¬
            nlu_response = self.nlu_engine.process_message(
                preprocessed_input, session_id, session_attributes
            )
            
            # ê°ì • ë¶„ì„
            sentiment_result = None
            if include_sentiment:
                sentiment_result = self._analyze_sentiment(preprocessed_input, detected_language)
            
            # ì—”í‹°í‹° ì¶”ì¶œ ë³´ê°•
            enhanced_entities = self._enhance_entity_extraction(
                preprocessed_input, nlu_response.intent_result.entities
            )
            
            # ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
            context_analysis = self._analyze_context(
                session_attributes or {}, nlu_response.intent_result
            )
            
            # ì‘ë‹µ ìƒì„±
            enhanced_response = self._enhance_response(
                nlu_response, sentiment_result, context_analysis
            )
            
            # ë¶„ì„ ê²°ê³¼ ì €ì¥
            self._save_analysis_result(
                session_id, user_input, nlu_response, sentiment_result,
                detected_language, enhanced_entities
            )
            
            # ë©”íŠ¸ë¦­ ì „ì†¡
            self._send_metrics(nlu_response.intent_result, sentiment_result)
            
            return {
                'success': True,
                'intent': nlu_response.intent_result.intent,
                'confidence': nlu_response.intent_result.confidence,
                'entities': enhanced_entities,
                'sentiment': sentiment_result,
                'language': detected_language,
                'response_text': enhanced_response,
                'next_action': nlu_response.next_action,
                'session_attributes': nlu_response.session_attributes,
                'context_analysis': context_analysis
            }
            
        except Exception as e:
            logger.error(f"NLU ì„œë¹„ìŠ¤ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'response_text': 'ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                'next_action': 'error'
            }
    
    def analyze_conversation_intent(self, conversation_history: List[Dict]) -> Dict[str, Any]:
        """
        ëŒ€í™” ì „ì²´ì˜ ì˜ë„ ë¶„ì„
        
        Args:
            conversation_history: ëŒ€í™” ì´ë ¥
            
        Returns:
            Dict: ëŒ€í™” ì˜ë„ ë¶„ì„ ê²°ê³¼
        """
        try:
            if not conversation_history:
                return {'primary_intent': 'unknown', 'confidence': 0.0}
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ë§Œ ì¶”ì¶œ
            user_messages = [
                msg['content'] for msg in conversation_history 
                if msg.get('source') == 'user'
            ]
            
            if not user_messages:
                return {'primary_intent': 'unknown', 'confidence': 0.0}
            
            # ê° ë©”ì‹œì§€ë³„ ì˜ë„ ë¶„ì„
            intent_scores = {}
            total_confidence = 0.0
            
            for message in user_messages:
                # ì„ì‹œ ì„¸ì…˜ìœ¼ë¡œ ì˜ë„ ë¶„ì„
                temp_session = f"analysis_{datetime.now().timestamp()}"
                result = self.nlu_engine.process_message(message, temp_session)
                
                intent = result.intent_result.intent
                confidence = result.intent_result.confidence
                
                if intent in intent_scores:
                    intent_scores[intent] += confidence
                else:
                    intent_scores[intent] = confidence
                
                total_confidence += confidence
            
            # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì˜ë„ ì„ íƒ
            if intent_scores:
                primary_intent = max(intent_scores, key=intent_scores.get)
                primary_confidence = intent_scores[primary_intent] / len(user_messages)
                
                return {
                    'primary_intent': primary_intent,
                    'confidence': primary_confidence,
                    'intent_distribution': intent_scores,
                    'message_count': len(user_messages)
                }
            
            return {'primary_intent': 'unknown', 'confidence': 0.0}
            
        except Exception as e:
            logger.error(f"ëŒ€í™” ì˜ë„ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {'primary_intent': 'error', 'confidence': 0.0}
    
    def get_intent_suggestions(self, partial_input: str, limit: int = 5) -> List[Dict[str, Any]]:
        """
        ë¶€ë¶„ ì…ë ¥ì— ëŒ€í•œ ì˜ë„ ì œì•ˆ
        
        Args:
            partial_input: ë¶€ë¶„ ì…ë ¥ í…ìŠ¤íŠ¸
            limit: ì œì•ˆ ê°œìˆ˜ ì œí•œ
            
        Returns:
            List[Dict]: ì˜ë„ ì œì•ˆ ëª©ë¡
        """
        try:
            # ì§€ì›ë˜ëŠ” ì˜ë„ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            supported_intents = self.nlu_engine.get_supported_intents()
            
            # ë¶€ë¶„ ì…ë ¥ê³¼ ìœ ì‚¬í•œ ì˜ë„ ì°¾ê¸°
            suggestions = []
            
            for intent in supported_intents:
                # ì˜ë„ëª…ê³¼ ìœ ì‚¬ë„ ê³„ì‚°
                similarity = self._calculate_string_similarity(partial_input, intent)
                
                if similarity > 0.3:  # ì„ê³„ê°’ ì´ìƒë§Œ í¬í•¨
                    suggestions.append({
                        'intent': intent,
                        'similarity': similarity,
                        'description': self._get_intent_description(intent)
                    })
            
            # ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            suggestions.sort(key=lambda x: x['similarity'], reverse=True)
            
            return suggestions[:limit]
            
        except Exception as e:
            logger.error(f"ì˜ë„ ì œì•ˆ ì˜¤ë¥˜: {str(e)}")
            return []
    
    def train_with_feedback(self, user_input: str, expected_intent: str,
                           actual_intent: str, session_id: str) -> bool:
        """
        í”¼ë“œë°±ì„ í†µí•œ í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘
        
        Args:
            user_input: ì‚¬ìš©ì ì…ë ¥
            expected_intent: ê¸°ëŒ€ëœ ì˜ë„
            actual_intent: ì‹¤ì œ ë¶„ì„ëœ ì˜ë„
            session_id: ì„¸ì…˜ ID
            
        Returns:
            bool: ì„±ê³µ ì—¬ë¶€
        """
        try:
            training_data = {
                'training_id': f"train_{datetime.now().timestamp()}",
                'user_input': user_input,
                'expected_intent': expected_intent,
                'actual_intent': actual_intent,
                'session_id': session_id,
                'created_at': datetime.now().isoformat(),
                'processed': False
            }
            
            self.training_table.put_item(Item=training_data)
            
            logger.info(f"í•™ìŠµ ë°ì´í„° ì €ì¥: {expected_intent} vs {actual_intent}")
            return True
            
        except Exception as e:
            logger.error(f"í•™ìŠµ ë°ì´í„° ì €ì¥ ì˜¤ë¥˜: {str(e)}")
            return False
    
    def get_nlu_analytics(self, start_date: str, end_date: str) -> Dict[str, Any]:
        """
        NLU ë¶„ì„ í†µê³„ ì¡°íšŒ
        
        Args:
            start_date: ì‹œì‘ ë‚ ì§œ
            end_date: ì¢…ë£Œ ë‚ ì§œ
            
        Returns:
            Dict: ë¶„ì„ í†µê³„
        """
        try:
            response = self.analytics_table.scan(
                FilterExpression='created_at BETWEEN :start AND :end',
                ExpressionAttributeValues={
                    ':start': start_date,
                    ':end': end_date
                }
            )
            
            analytics_data = response.get('Items', [])
            
            # í†µê³„ ê³„ì‚°
            total_requests = len(analytics_data)
            intent_distribution = {}
            confidence_scores = []
            sentiment_distribution = {}
            language_distribution = {}
            
            for item in analytics_data:
                # ì˜ë„ ë¶„í¬
                intent = item.get('intent', 'unknown')
                intent_distribution[intent] = intent_distribution.get(intent, 0) + 1
                
                # ì‹ ë¢°ë„ ì ìˆ˜
                confidence = item.get('confidence', 0.0)
                confidence_scores.append(confidence)
                
                # ê°ì • ë¶„í¬
                sentiment = item.get('sentiment', {}).get('sentiment', 'neutral')
                sentiment_distribution[sentiment] = sentiment_distribution.get(sentiment, 0) + 1
                
                # ì–¸ì–´ ë¶„í¬
                language = item.get('language', 'unknown')
                language_distribution[language] = language_distribution.get(language, 0) + 1
            
            # í‰ê·  ì‹ ë¢°ë„ ê³„ì‚°
            average_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0.0
            
            return {
                'total_requests': total_requests,
                'average_confidence': average_confidence,
                'intent_distribution': intent_distribution,
                'sentiment_distribution': sentiment_distribution,
                'language_distribution': language_distribution,
                'low_confidence_rate': len([c for c in confidence_scores if c < 0.7]) / total_requests * 100 if total_requests > 0 else 0
            }
            
        except Exception as e:
            logger.error(f"NLU ë¶„ì„ í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
            return {}
    
    def _preprocess_input(self, user_input: str) -> str:
        """ì…ë ¥ ì „ì²˜ë¦¬"""
        # ê³µë°± ì •ë¦¬
        processed = re.sub(r'\s+', ' ', user_input.strip())
        
        # ì´ëª¨í‹°ì½˜ ì²˜ë¦¬ (ì„ íƒì )
        processed = re.sub(r'[ğŸ˜€-ğŸ™ğŸ’€-ğŸ™ˆğŸ‘€-ğŸ”—]', '', processed)
        
        # ì—°ì†ëœ íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
        processed = re.sub(r'[!?]{2,}', '!', processed)
        
        return processed
    
    def _detect_language(self, text: str) -> str:
        """ì–¸ì–´ ê°ì§€"""
        try:
            if len(text.strip()) < 3:
                return self.default_language
            
            response = self.comprehend.detect_dominant_language(Text=text)
            languages = response.get('Languages', [])
            
            if languages:
                detected_lang = languages[0]['LanguageCode']
                confidence = languages[0]['Score']
                
                if confidence > 0.8 and detected_lang in self.supported_languages:
                    return detected_lang
            
            return self.default_language
            
        except Exception as e:
            logger.error(f"ì–¸ì–´ ê°ì§€ ì˜¤ë¥˜: {str(e)}")
            return self.default_language
    
    def _analyze_sentiment(self, text: str, language: str) -> Optional[Dict[str, Any]]:
        """ê°ì • ë¶„ì„"""
        try:
            if language not in ['ko', 'en']:  # Comprehend ì§€ì› ì–¸ì–´ í™•ì¸
                return None
            
            response = self.comprehend.detect_sentiment(
                Text=text,
                LanguageCode=language
            )
            
            sentiment = response.get('Sentiment', 'NEUTRAL')
            scores = response.get('SentimentScore', {})
            
            return {
                'sentiment': sentiment.lower(),
                'confidence': scores.get(sentiment.capitalize(), 0.0),
                'scores': {
                    'positive': scores.get('Positive', 0.0),
                    'negative': scores.get('Negative', 0.0),
                    'neutral': scores.get('Neutral', 0.0),
                    'mixed': scores.get('Mixed', 0.0)
                }
            }
            
        except Exception as e:
            logger.error(f"ê°ì • ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return None
    
    def _enhance_entity_extraction(self, text: str, basic_entities: Dict) -> Dict[str, Any]:
        """ì—”í‹°í‹° ì¶”ì¶œ ë³´ê°•"""
        enhanced_entities = basic_entities.copy()
        
        # ì „í™”ë²ˆí˜¸ ì¶”ì¶œ
        phone_pattern = r'01[0-9]-[0-9]{4}-[0-9]{4}'
        phone_matches = re.findall(phone_pattern, text)
        if phone_matches:
            enhanced_entities['phone_number'] = phone_matches[0]
        
        # ì´ë©”ì¼ ì¶”ì¶œ
        email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        email_matches = re.findall(email_pattern, text)
        if email_matches:
            enhanced_entities['email'] = email_matches[0]
        
        # ë‚ ì§œ íŒ¨í„´ ì¶”ì¶œ (ê°„ë‹¨í•œ ì˜ˆì‹œ)
        date_patterns = [
            r'(\d{4})-(\d{2})-(\d{2})',
            r'(\d{2})/(\d{2})/(\d{4})',
            r'(\d{1,2})ì›”\s*(\d{1,2})ì¼'
        ]
        
        for pattern in date_patterns:
            date_matches = re.findall(pattern, text)
            if date_matches:
                enhanced_entities['date'] = date_matches[0]
                break
        
        # ê¸ˆì•¡ ì¶”ì¶œ
        amount_pattern = r'(\d{1,3}(?:,\d{3})*)\s*ì›'
        amount_matches = re.findall(amount_pattern, text)
        if amount_matches:
            enhanced_entities['amount'] = amount_matches[0]
        
        return enhanced_entities
    
    def _analyze_context(self, session_attributes: Dict, intent_result: IntentResult) -> Dict[str, Any]:
        """ì»¨í…ìŠ¤íŠ¸ ë¶„ì„"""
        context_analysis = {
            'is_repeat_request': False,
            'escalation_indicators': [],
            'urgency_level': 'normal',
            'customer_journey_stage': 'inquiry'
        }
        
        # ë°˜ë³µ ìš”ì²­ ê°ì§€
        last_intent = session_attributes.get('last_intent')
        if last_intent and last_intent == intent_result.intent:
            context_analysis['is_repeat_request'] = True
        
        # ì—ìŠ¤ì»¬ë ˆì´ì…˜ ì§€í‘œ ê°ì§€
        escalation_keywords = ['ìƒë‹´ì›', 'ì‚¬ëŒ', 'ë§¤ë‹ˆì €', 'ì±…ì„ì', 'í™”ë‚˜', 'ì§œì¦']
        for keyword in escalation_keywords:
            if keyword in intent_result.entities.values():
                context_analysis['escalation_indicators'].append(keyword)
        
        # ê¸´ê¸‰ë„ ë¶„ì„
        urgency_keywords = ['ê¸´ê¸‰', 'ë‹¹ì¥', 'ì¦‰ì‹œ', 'ë¹¨ë¦¬']
        for keyword in urgency_keywords:
            if any(keyword in str(value) for value in intent_result.entities.values()):
                context_analysis['urgency_level'] = 'high'
                break
        
        # ê³ ê° ì—¬ì • ë‹¨ê³„ ì¶”ì •
        journey_mapping = {
            'greeting': 'initial',
            'product_inquiry': 'consideration',
            'reservation': 'decision',
            'complaint': 'post_purchase',
            'technical_support': 'post_purchase'
        }
        
        context_analysis['customer_journey_stage'] = journey_mapping.get(
            intent_result.intent, 'inquiry'
        )
        
        return context_analysis
    
    def _enhance_response(self, nlu_response: NLUResponse, 
                         sentiment_result: Optional[Dict],
                         context_analysis: Dict) -> str:
        """ì‘ë‹µ ê°œì„ """
        base_response = nlu_response.response_text
        
        # ê°ì •ì— ë”°ë¥¸ ì‘ë‹µ ì¡°ì •
        if sentiment_result:
            sentiment = sentiment_result.get('sentiment', 'neutral')
            
            if sentiment == 'negative':
                # ë¶€ì •ì  ê°ì •ì— ëŒ€í•œ ê³µê° í‘œí˜„ ì¶”ê°€
                base_response = f"ë¶ˆí¸ì„ ë¼ì³ë“œë ¤ ì£„ì†¡í•©ë‹ˆë‹¤. {base_response}"
            elif sentiment == 'positive':
                # ê¸ì •ì  ê°ì •ì— ëŒ€í•œ ê°ì‚¬ í‘œí˜„ ì¶”ê°€
                base_response = f"ë¬¸ì˜í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. {base_response}"
        
        # ë°˜ë³µ ìš”ì²­ì— ëŒ€í•œ ì¶”ê°€ ì•ˆë‚´
        if context_analysis.get('is_repeat_request'):
            base_response += "\n\nì¶”ê°€ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ìƒë‹´ì›ì—ê²Œ ì—°ê²°í•´ë“œë¦´ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤."
        
        # ê¸´ê¸‰ë„ì— ë”°ë¥¸ ìš°ì„  ì²˜ë¦¬ ì•ˆë‚´
        if context_analysis.get('urgency_level') == 'high':
            base_response = f"ê¸´ê¸‰í•œ ìš”ì²­ìœ¼ë¡œ ì ‘ìˆ˜ë˜ì—ˆìŠµë‹ˆë‹¤. {base_response}"
        
        return base_response
    
    def _save_analysis_result(self, session_id: str, user_input: str,
                            nlu_response: NLUResponse, sentiment_result: Optional[Dict],
                            language: str, entities: Dict):
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        try:
            analysis_data = {
                'analysis_id': f"nlu_{datetime.now().timestamp()}",
                'session_id': session_id,
                'user_input': user_input,
                'intent': nlu_response.intent_result.intent,
                'confidence': nlu_response.intent_result.confidence,
                'entities': entities,
                'sentiment': sentiment_result,
                'language': language,
                'created_at': datetime.now().isoformat()
            }
            
            self.analytics_table.put_item(Item=analysis_data)
            
        except Exception as e:
            logger.error(f"ë¶„ì„ ê²°ê³¼ ì €ì¥ ì˜¤ë¥˜: {str(e)}")
    
    def _send_metrics(self, intent_result: IntentResult, sentiment_result: Optional[Dict]):
        """ë©”íŠ¸ë¦­ ì „ì†¡"""
        try:
            # ì˜ë„ ë¶„ì„ ë©”íŠ¸ë¦­
            self.cloudwatch.put_metric_data(
                Namespace='AICC/NLU',
                MetricData=[
                    {
                        'MetricName': 'IntentAnalysis',
                        'Value': 1,
                        'Unit': 'Count',
                        'Dimensions': [
                            {'Name': 'Intent', 'Value': intent_result.intent},
                            {'Name': 'Confidence', 'Value': str(int(intent_result.confidence * 10) / 10)}
                        ]
                    }
                ]
            )
            
            # ê°ì • ë¶„ì„ ë©”íŠ¸ë¦­
            if sentiment_result:
                self.cloudwatch.put_metric_data(
                    Namespace='AICC/NLU',
                    MetricData=[
                        {
                            'MetricName': 'SentimentAnalysis',
                            'Value': 1,
                            'Unit': 'Count',
                            'Dimensions': [
                                {'Name': 'Sentiment', 'Value': sentiment_result['sentiment']}
                            ]
                        }
                    ]
                )
            
        except Exception as e:
            logger.error(f"ë©”íŠ¸ë¦­ ì „ì†¡ ì˜¤ë¥˜: {str(e)}")
    
    def _calculate_string_similarity(self, str1: str, str2: str) -> float:
        """ë¬¸ìì—´ ìœ ì‚¬ë„ ê³„ì‚° (ê°„ë‹¨í•œ Jaccard ìœ ì‚¬ë„)"""
        set1 = set(str1.lower())
        set2 = set(str2.lower())
        
        intersection = len(set1.intersection(set2))
        union = len(set1.union(set2))
        
        return intersection / union if union > 0 else 0.0
    
    def _get_intent_description(self, intent: str) -> str:
        """ì˜ë„ ì„¤ëª… ë°˜í™˜"""
        descriptions = {
            'greeting': 'ì¸ì‚¬ ë° ëŒ€í™” ì‹œì‘',
            'product_inquiry': 'ìƒí’ˆ ë¬¸ì˜ ë° ì •ë³´ ìš”ì²­',
            'complaint': 'ë¶ˆë§Œ ë° ì»´í”Œë ˆì¸',
            'reservation': 'ì˜ˆì•½ ë° ìŠ¤ì¼€ì¤„ë§',
            'cancel_request': 'ì·¨ì†Œ ìš”ì²­',
            'technical_support': 'ê¸°ìˆ  ì§€ì› ìš”ì²­',
            'payment_inquiry': 'ê²°ì œ ê´€ë ¨ ë¬¸ì˜'
        }
        
        return descriptions.get(intent, 'ê¸°íƒ€ ë¬¸ì˜') 